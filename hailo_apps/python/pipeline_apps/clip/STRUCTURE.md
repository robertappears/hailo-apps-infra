# CLIP Directory Structure

## Main Folder (clip/)
```
clip/
├── README.md                     # Application usage documentation
├── README_SETUP.md               # Complete setup documentation
├── STRUCTURE.md                  # This file - directory structure guide
├── __init__.py                   # Empty package initializer
├── clip_app.py                   # Entry point for the application
├── clip_pipeline.py              # GStreamer pipeline implementation
├── clip_text_utils.py            # Text encoding utilities (uses setup/ files)
├── text_image_matcher.py         # Singleton class for matching text/image embeddings
├── gui.py                        # GTK GUI for threshold control and text prompts
├── embeddings.json               # Generated (desk, keyboard, etc.) - optional
├── example_embeddings.json       # Generated (cat, dog, etc.) - optional
└── setup/                        # Place required files here (see below)
  # Place the following files here (not generated by this repository):
  # - clip_tokenizer.json (CLIP tokenizer)
  # - token_embedding_lut.npy (Token embedding lookup table)
  # - text_projection.npy (Text projection matrix)
```

## Key Components

1. **Core Application Files**:
   - `clip_app.py` - Main entry point with argument parsing
   - `clip_pipeline.py` - GStreamer pipeline with detection/tracking/CLIP inference
   - `gui.py` - GTK-based GUI for interactive control
   - `text_image_matcher.py` - Singleton for text-image similarity matching

2. **Text Encoding**:
   - `clip_text_utils.py` - Complete text encoding pipeline (tokenization → embedding → inference)
   - Uses files from `setup/` folder automatically

## Required Files (in `setup/` folder)

The following files are **not** generated by this repository. You must obtain them yourself:
- `clip_tokenizer.json` (CLIP tokenizer)
- `token_embedding_lut.npy` (Token embedding lookup table)
- `text_projection.npy` (Text projection matrix)

Refer to the official OpenAI CLIP and HuggingFace documentation for instructions on how to generate or download these files:
- [OpenAI CLIP GitHub](https://github.com/openai/CLIP)
- [HuggingFace Transformers Documentation](https://huggingface.co/docs/transformers/model_doc/clip)
- [CLIP Tokenizer Info](https://huggingface.co/openai/clip-vit-base-patch32)

**This repository no longer provides scripts for generating these files.**

4. **Configuration Files**:
   - `embeddings.json` & `example_embeddings.json` - Pre-computed text embeddings (optional)



### Step 4: Generate Sample Embeddings (Optional)
```bash
cd setup
python3 build_sample_embeddings_json.py
```

This creates **one JSON file** in the parent directory:
- **`example_embeddings.json`** - Sample embeddings with: cat, dog, person, car, tree, building

**Requirements:**
- All three files from Step 1-3 must exist
- Valid Hailo text encoder HEF file
- `hailo_platform` package installed

**Important:** Running this script **overwrites** existing `example_embeddings.json`. Back up custom embeddings first!

## Usage

### Running the Application

#### Basic Usage
```bash
# Default mode with example embeddings (no camera)
hailo-clip

# With person detection
hailo-clip --detector person

# With face detection
hailo-clip --detector face

# With custom embeddings file
hailo-clip --json-path my_embeddings.json

# With live camera
hailo-clip --input rpi --detector person
```

#### Advanced Options
```bash
# Disable runtime text prompts (faster startup)
hailo-clip --disable-runtime-prompts

# Adjust detection threshold
hailo-clip --detection-threshold 0.7

# Custom video with USB camera
hailo-clip --input usb --json-path office_objects.json
```

### Use in Code (Text Encoding)
```python
# clip_text_utils.py automatically uses setup/ folder
from clip_text_utils import run_text_encoder_inference

# Real-time text encoding with Hailo
text_features = run_text_encoder_inference(
    text="A photo of a cat",
    hef_path="clip_vit_b_32_text_encoder.hef",
    text_projection_path="setup/text_projection.npy"  # REQUIRED!
)

# Or use pre-computed embeddings from JSON
from text_image_matcher import text_image_matcher

text_image_matcher.load_embeddings('example_embeddings.json')
matches = text_image_matcher.match(image_embedding_np)
```

## File Purposes

### Core Application Files (in main folder)

| File | Purpose | Key Features |
|------|---------|--------------|
| `clip_app.py` | Entry point | Argument parsing, launches application |
| `clip_pipeline.py` | Pipeline manager | GStreamer pipeline with detection/tracking/CLIP inference |
| `gui.py` | User interface | GTK GUI for threshold control, text prompts, track ID focus |
| `text_image_matcher.py` | Matching engine | Singleton for text-image similarity, softmax scoring |
| `clip_text_utils.py` | Text encoding | Tokenization, embedding lookup, Hailo inference, postprocessing |

### Generated Core Files (in `setup/`)

| File | Size | Purpose | Generated By | Required |
|------|------|---------|--------------|----------|
| `clip_tokenizer.json` | ~3.5 MB | Text → Token IDs (vocabulary: 49408 tokens) | `generate_tokenizer.py` | Yes |
| `token_embedding_lut.npy` | ~97 MB | Token IDs → Embeddings (49408 × 512 for ViT-B/32) | `generate_token_embedding_lut_openai_clip.py` | Yes |
| `text_projection.npy` | ~1 MB | Encoder output → Final embeddings (512 × 512 for ViT-B/32) | `generate_text_projection_openai_clip.py` | Yes |

### Configuration Files (in main folder)

| File | Size | Purpose | Generated By | Required |
|------|------|---------|--------------|----------|
| `example_embeddings.json` | ~200 KB | Pre-computed text embeddings (cat, dog, etc.) | `build_sample_embeddings_json.py` | Optional |
| `embeddings.json` | Custom | User-defined text embeddings | GUI or custom script | Optional |

**Note:** The embeddings JSON format:
```json
{
  "threshold": 0.5,
  "text_prefix": "A photo of a ",
  "ensemble_template": ["a photo of a {}.", "a photo of the {}.", ...],
  "entries": [
    {
      "text": "cat",
      "embedding": [0.024, -0.063, ...],  // 512-dim for ViT-B/32
      "negative": false,
      "ensemble": false
    }
  ]
}
```

## Architecture Overview

### Application Flow

```
┌─────────────────┐
│   clip_app.py   │ ← Entry point
└────────┬────────┘
         │ creates
         ▼
┌──────────────────────────────────────┐
│       clip_pipeline.py               │ ← GStreamer pipeline
│  ┌────────────────────────────────┐  │
│  │ Detection → Tracking → CLIP    │  │
│  │ (optional)   (optional)        │  │
│  └────────────────────────────────┘  │
└───────┬──────────────────┬───────────┘
        │                  │
        │ uses             │ uses
        ▼                  ▼
┌──────────────────┐   ┌─────────────────┐
│ text_image_      │   │    gui.py       │
│ matcher.py       │   │  (GTK window)   │
│ (singleton)      │◄──┤  - Threshold    │
└──────────────────┘   │  - Text prompts │
                       │  - Track ID     │
                       └─────────────────┘
```

### Text Encoding Pipeline

```
Text String
    │
    ▼
┌───────────────────────────────────────────────┐
│          clip_text_utils.py                   │
│  ┌─────────────────────────────────────────┐  │
│  │ 1. Tokenize (clip_tokenizer.json)      │  │
│  │    "a photo of a cat" → [49406, 320,   │  │
│  │                           1125, ...]    │  │
│  ├─────────────────────────────────────────┤  │
│  │ 2. Lookup Embeddings                    │  │
│  │    (token_embedding_lut.npy)            │  │
│  │    [49406, 320, ...] → (1, 77, 512)    │  │
│  ├─────────────────────────────────────────┤  │
│  │ 3. Hailo Text Encoder Inference         │  │
│  │    Input: (1, 77, 512)                  │  │
│  │    Output: (1, 77, 512)                 │  │
│  ├─────────────────────────────────────────┤  │
│  │ 4. Extract EOT token + Project          │  │
│  │    (text_projection.npy)                │  │
│  │    (1, 77, 512) → (1, 512)             │  │
│  ├─────────────────────────────────────────┤  │
│  │ 5. L2 Normalize                         │  │
│  │    (1, 512) → normalized embedding      │  │
│  └─────────────────────────────────────────┘  │
└───────────────────────────────────────────────┘
                    │
                    ▼
           Text Embedding (1, 512)
```

### Pipeline Modes

#### Mode 1: Direct CLIP (--detector none)
```
Video → CLIP Image Encoder → Matching → Display
```

#### Mode 2: With Detection (--detector person/vehicle/face/license-plate)
```
Video → Detection → Tracking → Cropping → CLIP → Matching → Display
```

## Key Features

### Text Image Matcher
- **Singleton pattern**: One instance shared across application
- **Softmax or normalized scoring**: Configurable similarity computation
- **Negative prompts**: Support for "not X" matching
- **Ensemble templates**: Multiple prompt variations for better accuracy
- **Track ID focus**: Per-object confidence tracking in video
- **JSON persistence**: Save/load text embeddings

### GUI Controls
- **Threshold slider**: Real-time adjustment (0.0 - 1.0)
- **Text prompts**: Up to 6 editable text boxes
- **Negative checkbox**: Mark prompts as negative matches
- **Ensemble checkbox**: Enable ensemble template matching
- **Track ID focus**: Display confidence for specific tracked object
- **Load/Save buttons**: Manage embedding JSON files
- **Progress bars**: Visual confidence feedback per prompt

### Detection Modes
- **none**: Direct CLIP on full frames
- **person**: Detect and classify persons (COCO class 1)
- **vehicle**: Detect and classify vehicles (COCO class 2)
- **face**: Detect and classify faces (COCO class 3)
- **license-plate**: Detect and classify license plates (COCO class 4)

## Workflow

### 1. Initial Setup (One-Time)
```bash
cd setup
python3 generate_tokenizer.py
python3 generate_token_embedding_lut_openai_clip.py
python3 generate_text_projection_openai_clip.py

# Optional: Generate sample embeddings
python3 build_sample_embeddings_json.py
```

### 2. Development
- Modify `build_sample_embeddings_json.py` to customize sample texts
- Re-run to regenerate `example_embeddings.json`
- Or use GUI to create/edit embeddings interactively

### 3. Runtime Options

**Option A: Pre-computed embeddings (faster startup)**
```bash
hailo-clip --json-path embeddings.json --disable-runtime-prompts
```
- Loads embeddings from JSON
- No text encoder initialization
- Faster startup

**Option B: Runtime text encoding (more flexible)**
```bash
hailo-clip --json-path embeddings.json
```
- Loads initial embeddings from JSON
- Initializes text encoder for live updates
- GUI allows real-time prompt editing

## Benefits

✓ Clean separation of setup vs runtime code
✓ Generated files organized in one place
✓ Easy to gitignore the entire setup/ folder if needed
✓ Clear documentation in main folder
✓ Pre-computed embeddings for faster runtime performance
✓ Interactive GUI for real-time prompt tuning
✓ Flexible detection modes for different use cases
✓ Singleton matcher prevents duplicate state

## Important Notes

### Critical for Text Encoding
⚠️ **ALWAYS provide `text_projection_path` when calling `run_text_encoder_inference()`**
```python
# CORRECT ✓
embedding = run_text_encoder_inference(
    text="cat",
    hef_path="clip_vit_b_32_text_encoder.hef",
    text_projection_path="setup/text_projection.npy"  # REQUIRED!
)

# WRONG ✗ - Results in incorrect embeddings
embedding = run_text_encoder_inference(
    text="cat",
    hef_path="clip_vit_b_32_text_encoder.hef"
)
```

### File Naming Convention
- Generator scripts use `_openai_clip` suffix (e.g., `generate_token_embedding_lut_openai_clip.py`)
- Generated files use simple names (e.g., `token_embedding_lut.npy`)
- This allows easy switching between OpenAI CLIP and other implementations

### Model Support
- **ViT-B/32** (default): 512-dim embeddings, 97MB token LUT
- **RN50x4**: 640-dim embeddings, different projection matrix
- All models use same tokenizer (49408 vocabulary)
